[{"authors":null,"categories":null,"content":"I am an engineer in the Geo-informatics and Space Technology Agency (GISTDA), Bangkok, Thailand. My research interests include remote sensing, computer vision. More specifically, I am interested in applying data analysis technique and machine learning to extract meaningful information from geo-spatial data.\n  Download my resum√©.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pnstst.github.io/author/panu-srestasathiern/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/panu-srestasathiern/","section":"authors","summary":"I am an engineer in the Geo-informatics and Space Technology Agency (GISTDA), Bangkok, Thailand. My research interests include remote sensing, computer vision. More specifically, I am interested in applying data analysis technique and machine learning to extract meaningful information from geo-spatial data.","tags":null,"title":"Panu Srestasathiern","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Panu Srestasathiern FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://pnstst.github.io/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://pnstst.github.io/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://pnstst.github.io/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://pnstst.github.io/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://pnstst.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Panu Srestasathiern","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more  Get Started  üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://pnstst.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":null,"categories":["R"],"content":"\r\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\r\rFigure 1: A fancy pie chart.\r\r\r","date":1606875194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606875194,"objectID":"bf1eb249db79f10ace7d22321494165a","permalink":"https://pnstst.github.io/post/2020-12-01-r-rmarkdown/","publishdate":"2020-12-01T21:13:14-05:00","relpermalink":"/post/2020-12-01-r-rmarkdown/","section":"post","summary":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Nattadet Vijaranakul","Saichon Jaiyen","Panu Srestasathiern","Siam Lawawirojwong","Kulsawad Jitkajornwanich"],"categories":null,"content":"","date":1592956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592956800,"objectID":"627df18dda780dbfbdb5a98595ad1177","permalink":"https://pnstst.github.io/publication/vijaranakul-2020/","publishdate":"2020-08-04T00:00:00Z","relpermalink":"/publication/vijaranakul-2020/","section":"publication","summary":"Since 2018 during the winter of every year (December - January), Thailand has been suffering from air pollution problems known as PM 2.5 toxic dust, affecting people daily lives especially in Bangkok and its metroplex. To cope with this problem, one of the traditional methods used is to implement physical air quality measurement devices at specific locations. Currently there are 21 stations across Bangkok and surrounding areas. Each station can assess air quality at the station point along with the given radius, meaning that areas far away from the station will not be assessed properly. In this paper, we propose a methodology that incorporates satellite images for air quality assessment with supervised machine learning techniques. Several classification models tested in this paper are Decision Tree, Naive Bayes, k-Nearest Neighbors (kNN), Random Forest, and Gradient Boosting. From our experiments, the highest performance model is Random Forest that has averaged accuracy of 0.914, averaged precision of 0.89, averaged recall of 0.814 and averaged F-1 score of 0.84825.","tags":[],"title":"Air quality assessment based on Landsat 8 images using supervised machine learning techniques","type":"publication"},{"authors":["Teerapong Panboonyuen","Kulsawasd Jitkajornwanich","Siam Lawawirojwong","Panu Srestasathiern","Peerapon Vateekul"],"categories":null,"content":"","date":1586649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586649600,"objectID":"14e4efa0124002d1ec3103fbd6a70955","permalink":"https://pnstst.github.io/publication/panboonyuen-2020/","publishdate":"2020-04-12T00:00:00Z","relpermalink":"/publication/panboonyuen-2020/","section":"publication","summary":"One of the fundamental tasks in remote sensing is the semantic segmentation on the aerial and satellite images. It plays a vital role in applications, such as agriculture planning, map updates, route optimization, and navigation. The state-of-the-art model is the Enhanced Global Convolutional Network (GCN152-TL-A) from our previous work. It composes two main components: (i) the backbone network to extract features and (ii) the segmentation network to annotate labels. However, the accuracy can be further improved, since the deep learning network is not designed for recovering low-level features (e.g., river, low vegetation). In this pape, we aim to improve the semantic segmentation network in three aspects, designed explicitly for the remotely sensed domain. First, we propose to employ a modern backbone network called ?High-Resolution Representation (HR) to extract features with higher quality. It repeatedly fuses the representations generated by the high-to-low subnetworks with the restoration of the low-resolution representations to the same depth and level. Second, Feature Fusion (FF) is added to our network to capture low-level features (e.g., lines, dots, or gradient orientation). It fuses between the features from the backbone and the segmentation models, which helps to prevent the loss of these low-level features. Finally, ?Depthwise Atrous Convolution (DA)? is introduced to refine the extracted features by using four multi-resolution layers in collaboration with a dilated convolution strategy. The experiment was conducted on three data sets: two private corpora from Landsat-8 satellite and one public benchmark from the ISPRS Vaihingen challenge. There are two baseline models: the Deep Encoder-Decoder Network (DCED) and our previous model. The results show that the proposed model significantly outperforms all baselines. It is the winner in all data sets and exceeds more than 90% of F1 : 0.9114, 0.9362, and 0.9111 in two Landsat-8 and ISPRS Vaihingen data sets, respectively. Furthermore, it achieves an accuracy beyond 90% on almost all classes.","tags":[],"title":"Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution","type":"publication"},{"authors":["Nathachai Thongniran","Kulsawasd Jitkajornwanich","Siam Lawawirojwong","Panu Srestasathiern","Peerapon Vateekul"],"categories":null,"content":"","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"e02ed4a8b3d2cd700231747a98b73341","permalink":"https://pnstst.github.io/publication/thongniran-2019b/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/publication/thongniran-2019b/","section":"publication","summary":"Lately, CNN-GRU demonstrates the ability of deep learning techniques on ocean surface current prediction. Improvement of the current prediction model creates positive impact on variety of marine activities, such as search-and-rescue, disaster monitoring and power forecasting. Deep learning techniques was successfully deployed to improve model performance in many areas due to their ability to handle enormous amounts of information in a variety of inputs and their huge growth in recent years. Latest ocean current prediction employed a combination of two mature techniques, which are Convolutional Neural Network (CNN) and Gated Recurrent Unit (GRU), to capture spatial and temporal characteristics of its nature. However, there is still room for improvement due to many modern techniques that still have not been employed, and domain knowledge in oceanic, such as lunar illumination, is not taken into account to improve prediction performance. This paper introduces the ocean surface prediction model that employs soft attention mechanism, transfer learning, and incorporation of domain knowledge inputs which are month number, lunar effect, and hour number. An experimental dataset from 2014 to 2016, provided by GISTDA, is collected by using high frequency (HF) radar stations located along the coastal Gulf of Thailand. The experiment compares an existing CNN-GRU and our proposed model. The result shows an improvement of the prediction model in terms of RMSE by 2.57%, and 3.44% on U and V components.","tags":[],"title":"Combining attentional CNN and GRU networks for ocean current prediction based on HF radar observations","type":"publication"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Charts Academic supports the popular Plotly chart format.\nSave your Plotly JSON in your page folder, for example chart.json, and then add the {{\u0026lt; chart data=\u0026quot;chart\u0026quot; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\n  (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./line-chart.json\", function(chart) { Plotly.plot('chart-897563142', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  You might also find the Plotly JSON Editor useful.\nMath Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$  renders as\n$$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\\n1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Callouts Academic supports a shortcode for callouts, also referred to as asides, hints, or alerts. By wrapping a paragraph in {{% callout note %}} ... {{% /callout %}}, it will render as an aside.\n{{% callout note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /callout %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Spoilers Add a spoiler to a page to reveal text, such as an answer to a question, after a button is clicked.\n{{\u0026lt; spoiler text=\u0026quot;Click to view the spoiler\u0026quot; \u0026gt;}} You found me! {{\u0026lt; /spoiler \u0026gt;}}  renders as\nClick to view the spoiler You found me!\n Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it üôå ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://pnstst.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Nathachai Thongniran","Peerapon Vateekul","Kulsawasd Jitkajornwanich","Siam Lawawirojwong","Panu Srestasathiern"],"categories":null,"content":"","date":1562716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562716800,"objectID":"55a4484c27183a63c7face157d4dc94e","permalink":"https://pnstst.github.io/publication/thongniran-2019/","publishdate":"2019-07-10T00:00:00Z","relpermalink":"/publication/thongniran-2019/","section":"publication","summary":"Ocean surface current prediction is necessary to carry a variety of marine activities, such as disaster monitoring, search and rescue operations, etc. There are three traditional forecasting approaches: (i) numerical based approach, (ii) time series based approach and (iii) machine learning based approach. Unfortunately, their prediction accuracy was limited since they did not cooperate with spatial and temporal effects together. In this paper, we present a novel current prediction model, which is a combination between Convolutional Neural Network (CNN) to extract spatial characteristic and Gated Recurrent Unit (GRU) to find a relationship of temporal characteristic. The dataset is collected by high frequency (HF) radar stations located along coastal Thailands gulf by GISTDA from 2014 to 2016. It was an intensive experiment comparing our method and eight existing methods, e.g., ARIMA, kNN, Perceptron, Multilayer Perceptron (MLP), etc. The results show that our network outperforms almost all baselines in terms of RMSE for 11.21% and 27.01% averaging improvement on U and V components, consecutively.","tags":[],"title":"Spatio-temporal deep learning for ocean current prediction based on HF radar data","type":"publication"},{"authors":["Panu Srestasathiern"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')     print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://pnstst.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://pnstst.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Teerapong Panboonyuen","Kulsawasd Jitkajornwanich","Siam Lawawirojwong","Panu Srestasathiern","Peerapon Vateekul"],"categories":null,"content":"","date":1546560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546560000,"objectID":"9353308bf13732ae170d5271712d3f88","permalink":"https://pnstst.github.io/publication/panboonyuen-2019/","publishdate":"2019-01-04T00:00:00Z","relpermalink":"/publication/panboonyuen-2019/","section":"publication","summary":"In the remote sensing domain, it is crucial to complete semantic segmentation on the raster images, eg, river, building, forest, etc., on raster images. A deep convolutional encoder?decoder (DCED) network is the state-of-the-art semantic segmentation method for remotely sensed images. However, the accuracy is still limited, since the network is not designed for remotely sensed images and the training data in this domain is deficient. In this paper, we aim to propose a novel CNN for semantic segmentation particularly for remote sensing corpora with three main contributions. First, we propose applying a recent CNN called a global convolutional network (GCN), since it can capture different resolutions by extracting multi-scale features from different stages of the network. Additionally, we further enhance the network by improving its backbone using larger numbers of layers, which is suitable for medium resolution remotely sensed images. Second,?channel attention? is presented in our network in order to select the most discriminative filters (features). Third,?domain-specific transfer learning? is introduced to alleviate the scarcity issue by utilizing other remotely sensed corpora with different resolutions as pre-trained data. The experiment was then conducted on two given datasets:(i) medium resolution data collected from Landsat-8 satellite and (ii) very high resolution data called the ISPRS Vaihingen Challenge Dataset. The results show that our networks outperformed DCED in terms of F 1 for 17.48% and 2.49% on medium and very high resolution corpora, respectively. ","tags":[],"title":"Semantic segmentation on remotely sensed images using an enhanced global convolutional network with channel attention and domain specific transfer learning","type":"publication"},{"authors":["Kulsawasd Jitkajornwanich","Chanwit Kongthong","Nattaya Khongsoontornjaroen","Jeedapa Kaiyasuan","Siam Lawawirojwong","Panu Srestasathiern","Siwapon Srisonphan","Peerapon Vateekul"],"categories":null,"content":"","date":1544400000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544400000,"objectID":"f153ce003e64b44719835f6f182efc2a","permalink":"https://pnstst.github.io/publication/jitkajornwanich-2018/","publishdate":"2018-12-10T00:00:00Z","relpermalink":"/publication/jitkajornwanich-2018/","section":"publication","summary":"Natural disasters cause significant damage to the country as well as its citizens as we have seen in the news. Drought, wild fire, earthquake and flooding are some examples of the primary natural disasters occurred in Thailand. In this research, we focus on flooding and use data from Twitter, where users mobile devices are utilized as IoT input channels. The goal of this work is to analyze near real-time data (tweets) for early flood warning. Traditional methods in processing, analyzing and reporting a flooding event take quite some time. In social medias (through cellphones), on the other hand, by harvesting crowdsources, potential flooding can be predicted faster-though with the price of reliability of the retrieved tweets. In our research, several techniques are incorporated in order to maximize the accuracy of results, including, tokenization, geo-encoding and decoding, NLP via string matching (Levenshteins algorithms), and Google APIs for visualization. Finally, the dynamic yet user-friendly map is produced with respect to the posted relevant tweets along their associated frequencies.","tags":[],"title":"Utilizing twitter data for early flood warning in Thailand","type":"publication"},{"authors":["Sitthisak Moukomla","Panu Srestasathiern","Suramongkon Siripon","Rattawat Wasuhiranyrith","Phalakorn Kooha"],"categories":null,"content":"","date":1539129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539129600,"objectID":"44b307958c5f705ebb707b5883bf579b","permalink":"https://pnstst.github.io/publication/moukomla-2018/","publishdate":"2018-10-10T00:00:00Z","relpermalink":"/publication/moukomla-2018/","section":"publication","summary":"The fast-growing Eucalyptus trees are important for a renewable energy source, carbon storage as well as the variety of industry. For Eucalyptus tree plantation inventory and monitoring, Canopy Height (CH) and Above Ground Biomass (AGB) are required parameters. However, the traditional inventory methods are label intensive, timeconsuming, and inaccurate. This study, we presented the method for estimating CH and AGB from Eucalyptus plantation based on the very high spatial resolution imagery (~5 cm.) from Unmanned Aircraft Vehicle (UAV). To estimate AGB, we first generated the digital surface models (DSMs) from 3 D point-cloud. Then, the digital terrain models (DTMs) were generated from Real Time Kinematic (RTK) Differential Global Positioning System (DGPS) surveying technique. The CH was calculated based on the different of DSM and DTM. We located individual trees location using tree detection software. Next, the allometric equation was generated from the 20 samples (tree) and their subsamples (e.g. foliage, litter, branch). The correlation between sampling tree height and tree weight was relatively high (R^2 = 0.977) with the exponential relationship. We then apply the correlation to estimate tree weight of the remaining trees. Next, the individual UAVs-based AGB was calculated based on the ratio of fresh weight and dry weight. The total AGB was 3450 kg. per ha.","tags":[],"title":"Estimating above ground biomass for eucalyptus plantation using data from unmanned aerial vehicle imagery","type":"publication"},{"authors":["Sirinthra Chantharaj","Kissada Pornratthanapong","Pitchayut Chitsinpchayakun","Teerapong Panboonyuen","Peerapon Vateekul","Siam Lawavirojwong","Panu Srestasathiern","Kulsawasd Jitkajornwanich"],"categories":null,"content":"","date":1536537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536537600,"objectID":"237e379e7945f012f0dc143a1fcad664","permalink":"https://pnstst.github.io/publication/chantharaj-2018/","publishdate":"2018-09-10T00:00:00Z","relpermalink":"/publication/chantharaj-2018/","section":"publication","summary":"Semantic Segmentation is a fundamental task in computer vision and remote sensing imagery. Many applications, such as urban planning, change detection, and environmental monitoring, require the accurate segmentation; hence, most segmentation tasks are performed by humans. Currently, with the growth of Deep Convolutional Neural Network (DCNN), there are many works aiming to find the best network architecture fitting for this task. However, all of the studies are based on very-high resolution satellite images, and surprisingly; none of them are implemented on medium resolution satellite images. Moreover, no research has applied geoinformatics knowledge. Therefore, we purpose to compare the semantic segmentation models, which are FCN, SegNet, and GSN using medium resolution images from Landsat-8 satellite. In addition, we propose a modified SegNet model that can be used with remote sensing derived indices. The results show that the model that achieves the highest accuracy RGB bands of medium resolution aerial imagery is SegNet. The overall accuracy of the model increases when includes Near Infrared (NIR) and Short-Wave Infrared (SWIR) band. The results showed that our proposed method (our modified SegNet model, named RGB-IR-IDX-MSN method) outperforms all of the baselines in terms of mean F1 scores.","tags":[],"title":"Semantic segmentation on medium-resolution satellite images using deep convolutional networks with remote sensing derived indices","type":"publication"},{"authors":["Chanika Sukawattanavijit","Panu Srestasathiern"],"categories":null,"content":"","date":1509580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509580800,"objectID":"39cbbfc5d3e60b876d1c0925cda9a81a","permalink":"https://pnstst.github.io/publication/sukawattanavijit-2017/","publishdate":"2017-11-02T00:00:00Z","relpermalink":"/publication/sukawattanavijit-2017/","section":"publication","summary":"Land Use and Land Cover (LULC) information are significant to observe and evaluate environmental change. LULC classification applying remotely sensed data is a technique popularly employed on a global and local dimension particularly, in urban areas which have diverse land cover types. These are essential components of the urban terrain and ecosystem. In the present, object-based image analysis (OBIA) is becoming widely popular for land cover classification using the high-resolution image. COSMO-SkyMed SAR data was fused with THAICHOTE (namely, THEOS: Thailand Earth Observation Satellite) optical data for land cover classification using object-based. This paper indicates a comparison between object-based and pixel-based approaches in image fusion. The per-pixel method, support vector machines (SVM) was implemented to the fused image based on Principal Component Analysis (PCA). For the objectbased classification was applied to the fused images to separate land cover classes by using nearest neighbor (NN) classifier. Finally, the accuracy assessment was employed by comparing with the classification of land cover mapping generated from fused image dataset and THAICHOTE image. The object-based data fused COSMO-SkyMed with THAICHOTE images demonstrated the best classification accuracies, well over 85%. As the results, an object-based data fusion provides higher land cover classification accuracy than per-pixel data fusion.","tags":[],"title":"Object-based land cover classification based on fusion of multifrequency SAR data and THAICHOTE optical imagery","type":"publication"},{"authors":["Gabor Barsai","Alper Yilmaz","Sudhagar Nagarajan","Panu Srestasathiern"],"categories":null,"content":"","date":1507420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507420800,"objectID":"ceba9c7eb8fd2b78a1a03571345cc621","permalink":"https://pnstst.github.io/publication/barsai-2017/","publishdate":"2017-10-08T00:00:00Z","relpermalink":"/publication/barsai-2017/","section":"publication","summary":"Recovering the camera orientation is a fundamental problem in photogrammetry for precision 3D recovery, orthophoto generation, and image registration. In this paper, we achieve this goal by fusing the image information with information extracted from different modalities, including lidar and GIS. In contrast to other approaches, which require feature correspondences, our approach exploits edges across the modalities without the necessity to explicitly establish correspondences. In the proposed approach, extracted edges from different modalities are not required to have analytical forms. This flexibility is achieved by minimizing a new cost function using a Bayesian approach, which takes the Euclidean distances between the projected edges extracted from the other data source and the edges extracted from the reference image as its random variable. The proposed formulation minimizes the overall distances between the sets of edges iteratively, such that the end product results in the correct camera parameters for the reference image as well as matching features across the modalities. The initial solution can be obtained from GPS/IMU data. The formulation is shown to successfully handle noise and missing observations in edges. Point matching methods may fail for oblique images, especially high oblique images. We eliminate the requirement for exact point-to-point matching. The feasibility of the method is experimented with nadir and oblique images.","tags":[],"title":"Registration of images to Lidar and GIS data without establishing explicit correspondences","type":"publication"},{"authors":["Tewan Santitewagun","Panu Srestasathiern","Pakapoj Tulsuk","Miti Ruchanurucks","Teera Phatrapornnant","Shoichi Hasegawa"],"categories":null,"content":"","date":1498435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498435200,"objectID":"1d569556fba23ca62b4e7533e7286226","permalink":"https://pnstst.github.io/publication/santitewagun-2017/","publishdate":"2017-06-26T00:00:00Z","relpermalink":"/publication/santitewagun-2017/","section":"publication","summary":"This paper presents an improvement for error reduction of the cost function for non-linear optimization of extrinsic parameters estimation between single line scan LiDAR and RGB camera. The non-linear optimization utilizes a least square scheme by assigning equal weights to all LiDAR measurement points. With robust regression, we used all LiDAR measurement points and removed RANSAC outlier removal with a weighting scheme dependent on the defined geometric constraint. The methods aims to minimize the error from the inaccuracy of the LiDAR measurement points using robust regression with M-estimator. The methods are tested with 100 random trials with noise magnitude from 5 to 40mm and a 10 percent chance of outliers of 3 times the normal noise magnitude. The results show that M-estimator is more resistant to noise than current state of art.","tags":[],"title":"Robust regression in extrinsic calibration between camera and single line scan laser rangefinder","type":"publication"},{"authors":["Chaiyaporn Kitpracha","Darunee Promchot","Panu Srestasathiern","Chalermchon Satirapod"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"2e499ff043436a64c8fc10857234ddd5","permalink":"https://pnstst.github.io/publication/kitpracha-2017/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/kitpracha-2017/","section":"publication","summary":"It is well known that the atmospheric effects are the most dominant spatially correlated errors in GNSS observations. The atmosphere causing the delay in GNSS observations consists of two main layers i.e. ionosphere and troposphere. The ionospheric delay can be mitigated using a linear combination of the two-frequency data. Unlike the ionospheric delay, the tropospheric delay cannot be removed using the same methodology. Compensation for the tropospheric delay is usually carried out using a standard tropospheric model. However, a standard tropospheric model cannot be used to completely remove the tropospheric delay in GNSS observations. With the use of GNSS Precise Point Positioning (PPP) technique, the tropospheric delay can be precisely obtained in a data processing step. Therefore, this paper focuses on an estimation of precise tropospheric delay using the GNSS PPP technique with the Positioning and Navigation Data Analyst (PANDA) software. Results obtained from the PPP technique are then compared with IGS tropospheric products and standard tropospheric models e.g. Saastamonien, modified Hopfield and simplified Hopfield. The obtained tropospheric delay shows a good agreement with the IGS product at millimeter level. It is found that the standard tropospheric models can only provide tropospheric corrections which are accurate at decimeter level in Thailand region. Finally, the same processing procedure is applied to produce precise tropospheric delays for all GNSS Continuously Operating Reference Stations (CORS) in Thailand and initial tropospheric delay results are presented in this paper.","tags":[],"title":"Precise tropospheric delay map of Thailand using GNSS precise point positioning technique","type":"publication"},{"authors":["Teerapong Panboonyuen","Kulsawasd Jitkajornwanich","Siam Lawawirojwong","Panu Srestasathiern","Peerapon Vateekul"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"497f899a59a58e928ef008972c038f73","permalink":"https://pnstst.github.io/publication/panboonyuen-2017/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/panboonyuen-2017/","section":"publication","summary":"Object segmentation of remotely-sensed aerial (or very-high resolution, VHS) images and satellite (or high-resolution, HR) images, has been applied to many application domains, especially in road extraction in which the segmented objects are served as a mandatory layer in geospatial databases. Several attempts at applying the deep convolutional neural network (DCNN) to extract roads from remote sensing images have been made; however, the accuracy is still limited. In this paper, we present an enhanced DCNN framework specifically tailored for road extraction of remote sensing images by applying landscape metrics (LMs) and conditional random fields (CRFs). To improve the DCNN, a modern activation function called the exponential linear unit (ELU), is employed in our network, resulting in a higher number of, and yet more accurate, extracted roads. To further reduce falsely classified road objects, a solution based on an adoption of LMs is proposed. Finally, to sharpen the extracted roads, a CRF method is added to our framework. The experiments were conducted on Massachusetts road aerial imagery as well as the Thailand Earth Observation System (THEOS) satellite imagery data sets. The results showed that our proposed framework outperformed Segnet, a state-of-the-art object segmentation technique, on any kinds of remote sensing imagery, in most of the cases in terms of precision, recall, and F1.","tags":[],"title":"Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields","type":"publication"},{"authors":["Panu Srestasathiern"],"categories":null,"content":"","date":1494547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494547200,"objectID":"31d77742b413fc782db59d20b868584a","permalink":"https://pnstst.github.io/publication/srestasathiern-2017/","publishdate":"2017-05-12T00:00:00Z","relpermalink":"/publication/srestasathiern-2017/","section":"publication","summary":"With the emerging of mobile surveying technology such as UAV and mobile mapping system, the data collection for GIS is becoming more affordable. The sensors that commonly installed on platforms are cameras. In order to obtain accurate geometric information such as positions, heights, or areas of 3D object from images, the geometric camera calibration is very important.","tags":[],"title":"Geometric Camera Calibration in Support of GIS","type":"publication"},{"authors":["Rata Suwantong","Panu Srestasathiern","Chalermchon Satirapod","Shi Chuang","Chaiyaporn Kitpracha"],"categories":null,"content":"","date":1486425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486425600,"objectID":"c74de5f6640ccb377d17dd6d1b44b489","permalink":"https://pnstst.github.io/publication/suwantong-2017/","publishdate":"2017-02-07T00:00:00Z","relpermalink":"/publication/suwantong-2017/","section":"publication","summary":"In this paper, the problem of modeling the relationship between the mean atmospheric and air surface temperatures is addressed. Particularly, the major goal is to estimate the model parameters at a regional scale in Thailand. To formulate the relationship between the mean atmospheric and air surface temperatures, a triply modulated cosine function was adopted to model the surface temperature as a periodic function. The surface temperature was then converted to mean atmospheric temperature using a linear function. The parameters of the model were estimated using an extended Kalman filter. Traditionally, radiosonde data is used. In this paper, satellite data from an atmospheric infrared sounder, and advanced microwave sounding unit sensors was used because it is open source data and has global coverage with high temporal resolution. The performance of the proposed model was tested against that of a global model via an accuracy assessment of the computed GNSS-derived PWV.","tags":[],"title":"Mean atmospheric temperature model estimation for GNSS meteorology using AIRS and AMSU data","type":"publication"},{"authors":["Rata Suwantong","Chalermchon Satirapod","Panu Srestasathiern","Chaiyaporn Kitpracha"],"categories":null,"content":"","date":1473638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473638400,"objectID":"3db3e34b55ca11e5800b07ce624279a9","permalink":"https://pnstst.github.io/publication/suwantong-2016c/","publishdate":"2016-09-12T00:00:00Z","relpermalink":"/publication/suwantong-2016c/","section":"publication","summary":"Accurate estimation of the Precipitable Water Vapour (PWV) is hence crucial for weather forecast and climate studies. The Global Navigation Satellite System (GNSS) data is generally used for continuous, accurate, all-weather and realtime PWV. For this, one has to compute the mean tropospheric temperature which is generally modelled as a linear function depending on the air surface temperature. The parameters for a mean tropospheric temperature model are usually derived using radiosonde data which is expensive and covers only a small area. In this work, we propose to use temperature and water vapour profiles from satellite data to derive parameters of the mean tropospheric temperature models using the Extended Kalman Filter (EKF) supposing that the air surface temperature is periodic. The satellite data is obtained from the Atmospheric Infrared Sounder (AIRS) and the Advanced Microwave Sounding Unit (AMSU) board on NASA Aqua satellite and available for free. The satellite has also global coverage and passes over a specific region on Earth twice a day and hence appropriate for local and global mean tropospheric temperature model derivation. We also show that the mean error of the derived PWV is reduced when the derived local model is used compared to when a global model is used for PWV estimation in Bangkok from January to November 2015.","tags":[],"title":"Deriving the mean tropospheric temperature model using AIRS and AMSU for GNSS precipitable water vapour estimation","type":"publication"},{"authors":["Rata Suwantong","Panu Srestasathiern","Siam Lawawirojwong","Preesan Rakwatin"],"categories":null,"content":"","date":1473292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473292800,"objectID":"b05c868f7bf0f0227ab88144eaa08aee","permalink":"https://pnstst.github.io/publication/suwantong-2016/","publishdate":"2016-09-08T00:00:00Z","relpermalink":"/publication/suwantong-2016/","section":"publication","summary":"Crop yield forecasting is important either for a government, agricultural industries or a trading company for their action plans. A very important variable to be given to a crop model used for the forecast is an accurate crop cultivation date. When the area of interest is large, it is preferable to use remote sensing data such as satellite images for the crop monitoring. The estimation of the cultivation date can be done using the Normalized Difference Vegetation Index (NDVI) collected from the Moderate Resolution Imaging Spectroradiometer (MODIS) aboard the Terra satellite which is interesting for a developing country thanks to its free and frequent availability. However, in a tropical area the NDVI data is very noisy due to clouds. To cope with this issue, it is proposed in the literature that one can model the NDVI by a triply modulated cosine function with the mean, the amplitude and the initial phase as state variables and then use the Extended Kalman Filter (EKF) to estimate these three variables. However, the EKF tuning and cultivation date computation methods in the literature cannot be used in reality for accurate age estimation. This paper therefore proposes to use the estimated total phase of the triply modulated cosine function to compute crop cultivation date. To specify, the crop cultivation date is defined as the day in which the estimated total phase is higher than a threshold. The threshold is predefined using available real cultivation dates from available ground truths during the same year in the region. Thanks to the method proposed in this paper, the mean cultivation date error is reduced from 11.70 days to 0.0 day and the mean absolute error is reduced from 26.2 days to 16.3 days compared to when the method in the literature is implemented for our study cases which are single crop rice fields in the northeast of Thailand.","tags":[],"title":"Accurate crop cultivation date estimation from MODIS using NDVI phases and the Extended Kalman Filter","type":"publication"},{"authors":["Panu Srestasathiern","Siam Lawawirojwong","Rata Suwantong"],"categories":null,"content":"","date":1473292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473292800,"objectID":"d28cc808444463842e5ba27bafe1632a","permalink":"https://pnstst.github.io/publication/srestasathiern-2016b/","publishdate":"2016-09-08T00:00:00Z","relpermalink":"/publication/srestasathiern-2016b/","section":"publication","summary":"Rice age estimation is an process in crop management and monitoring. In this paper, we present an approach for estimating age of rice on satellite image using support vector regression technique. The advantage of using support vector regression is that it is non-parametric technique. Therefore, any age model is not required. Moreover, the support vector regression is robust to outlier. The input data is the satellite image features i.e., radiometric information. To tune the parameters of support vector machine, the k-fold cross validation technique is utilized. The experiment was conducted using Landsat-8 image. Comparing the estimated age result with ground truth, the proposed method showed expected performance.","tags":[],"title":"Support vector regression for rice age estimation using satellite imagery","type":"publication"},{"authors":["Rata Suwantong","Panu Srestasathiern","Siam Lawawirojwong","Preesan Rakwatin"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"37a456c23d1ba69ac534278491ff4eb8","permalink":"https://pnstst.github.io/publication/suwantong-2016b/","publishdate":"2016-08-01T00:00:00Z","relpermalink":"/publication/suwantong-2016b/","section":"publication","summary":"Accurate crop start date estimation is crucial for crop yield forecasting which is important not only for a government but also for agriculture-based or trading companies. The estimation can be done using the Normalized Difference Vegetation Index (NDVI) computed from radiant energy from the crops of interest. The NDVI collected from the Moderate Resolution Imaging Spectroradiometer (MODIS) aboard the Terra satellite is chosen in this study thanks to its free availability which is suitable for a developing country. In a tropical country as Thailand, the NDVI data is very noisy due to high density of clouds. An appropriate estimation technique must therefore be implemented. In this paper, the NDVI is modelled by a triply modulated cosine function with the mean, the amplitude and the initial phase as state variables. The state and the NDVI of single rice crop in the northeast Thailand are estimated using the Extended Kalman Filter (EKF), the Unscented Kalman Filter (UKF), the Moving Horizon Estimator (MHE) and the Moving Horizon Estimator with Pre-Estimation (MHE-PE). The MHE-PE, recently proposed in the literature, is an optimization-based estimator using an auxiliary estimator to describe the dynamics of the state over the horizon which has been shown to overcome the classical MHE strategy in terms of accuracy and computation time. The EKF and the MHE-PE provide the smallest start date estimation error compared to the others, which is 0 day in mean and 18 days in standard deviation. However, the EKF fail to detect the NDVI of preplant crops and parasite weeds while the MHE-PE does not.","tags":[],"title":"Moving horizon estimator with pre-estimation for crop start date estimation in tropical area","type":"publication"},{"authors":["Panu Srestasathiern","Siam Lawawirojwong","Rata Suwantong","Pattrawuth Phuthong"],"categories":null,"content":"","date":1465257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465257600,"objectID":"1c0a4760f0ddfe5913d9a712ddb7fdc7","permalink":"https://pnstst.github.io/publication/srestasathiern-2016/","publishdate":"2016-06-07T00:00:00Z","relpermalink":"/publication/srestasathiern-2016/","section":"publication","summary":" This paper address the problem of rotation matrix sampling used for multidimensional probability distribution transfer. The distribution transfer has many applications in remote sensing and image processing such as color adjustment for image mosaicing, image classification, and change detection. The sampling begins with generating a set of random orthogonal matrix samples by Householder transformation technique. The advantage of using the Householder transformation for generating the set of orthogonal matrices is the uniform distribution of the orthogonal matrix samples. The obtained orthogonal matrices are then converted to proper rotation matrices. The performance of using the proposed rotation matrix sampling scheme was tested against the uniform rotation angle sampling. The applications of the proposed method were also demonstrated using two applications i.e., image to image probability distribution transfer and data Gaussianization.","tags":[],"title":"Rotation matrix sampling scheme for multidimensional probability distribution transfer","type":"publication"},{"authors":null,"categories":null,"content":"With fast development in geo-spatial data acquisition technology, a big load of data is obtained everyday. Moreover, the data over an area can be measure in a finer detail i.e., higher resolution satellite imagery. The object detail is therefore higher. The deep learning technology is suitable for extract object of interest on satellite imagery because it can learn the features that well describe the object.\n","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"cab323fbfd4d893cf69e7ea752b90641","permalink":"https://pnstst.github.io/project/deep-learning/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/project/deep-learning/","section":"project","summary":" ","tags":[],"title":"Deep learning for geo-spatial data analysis","type":"project"},{"authors":null,"categories":null,"content":"Precipitable water vapor is a crucial variable for weather forecast and climate studies. The signals of Global Navigational Satellite System (GNSS) satellites are used to extract the amount of precipitable water vapor in near-realtime. My research interest in this area is to derive model for estimating mean atmospheric temperature which is an input for extract the amount of precipitable water vapor from GNSS signals.\n","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"6b843340c73e8ce0976cc0822c6dea5f","permalink":"https://pnstst.github.io/project/precipitable-water-vapor/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/project/precipitable-water-vapor/","section":"project","summary":" ","tags":[],"title":"Precipitable water vapor estimation using GNSS","type":"project"},{"authors":["Panu Srestasathiern","Siam Lawawirojwong","Narut Soontranon","Preesan Rakwatin"],"categories":null,"content":"","date":1443657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443657600,"objectID":"9dd418e7b6e5ff092e348f18b75f5e53","permalink":"https://pnstst.github.io/publication/srestasathiern-2015/","publishdate":"2015-10-01T00:00:00Z","relpermalink":"/publication/srestasathiern-2015/","section":"publication","summary":"In this paper, we present a novel approach for unsupervised change detection on multi-spectral satellite images. The advantage of unsupervised approach over the supervised one is that the generation of an appropriated ground truth is not required. Especially, when the ground truth is not available, the unsupervised approach is the fundamental one. The unsupervised change detection method used in this paper is based on the concept of kernel Principal Component Analysis (PCA). The advantage of using kernel PCA over standard PCA is that it can handle non-linear relationships between data by projecting the data into higher dimension using kernel function. Assuming that relationship between data point after projecting data into higher dimensional space is linear, the principal basis vectors can then be extracted. A pixel is classified as unchanged if the absolute value of its second principal component is larger than a threshold. To improve the performance of kernel PCA, the ensemble approach is applied i.e., bootstrapped aggregating, which is also known as the bagging. The concept of bagging is to combine the change detection results from weak change detectors to improve overall performance. The proposed method begins with generating bootstrap samples. That is, the image pixels are random sampled with replacement. Each bootstrap sample is then used to construct a change detector using kernel PCA; a set of change detector is hence obtained. As a result, to classify a pixel as change or un-change, the change detection results from those change detectors are then fused by majority voting. By using random samples instead of using all image pixels at once for detecting change, the computation time is reduced. In other words, a large amount of computational resource is not required. To evaluated, the performance of the proposed methods, it was tested with multispectral satellite images i.e., Landsat 8. The experimental results will be illustrated as a change map. The analysis on principal basis vector extracted from the data using kernel PCA will be performed. ","tags":[],"title":"Unsupervised ensemble change detection using kernel PCA","type":"publication"},{"authors":["Pattrawuth Phuthong","Charay Lerdsudwuchai","Panu Srestasathiern"],"categories":null,"content":"","date":1440028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440028800,"objectID":"5a9b10224023c0989159452e4638e3e2","permalink":"https://pnstst.github.io/publication/phutong-2015/","publishdate":"2015-08-20T00:00:00Z","relpermalink":"/publication/phutong-2015/","section":"publication","summary":"Index generation for satellite image is an important task for retrieving satellite image in data warehouse. The goal of index generation is to find a polygon that represents the geographic image area. Using too many vertices for satellite images index can decrease retrieval speed. Therefore, the aim of this research is to develop an algorithm for generating index for satellite images using small amount of vertices and having high accuracy. The key idea of the algorithm is to decrease the number of vertices representing image shapes by mean of polygon simplification. The obtained polygon is expanded in order to make its cover the image area as much as possible. The experimental results showed that the proposed algorithm generated image index with high accuracy using small number of vertices comparing with other softwares i.e., ArcGIS 10.1, Global Mapper 15 and DIQB. In addition, the result is useful and quick for retrieving data from the spatial database.","tags":[],"title":"Index generation for satellite image retrieval","type":"publication"},{"authors":["Narut Soontranon","Panu Srestasathiern","Preesan Rakwatin"],"categories":null,"content":"","date":1440028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440028800,"objectID":"aaac2d8b5a2c6af0bc1f1d56cbf6b726","permalink":"https://pnstst.github.io/publication/soontranon-2015c/","publishdate":"2015-08-20T00:00:00Z","relpermalink":"/publication/soontranon-2015c/","section":"publication","summary":"In 2012, GISTDA has launched a sensor network project for monitoring agricultural fields in every region of Thailand. Integration of digital camera and weather sensors, Field Server (FS) is used to collect two types of data; image and weather condition. In this study, time-series images acquired from the rice field are used for computing and understanding the rice crop calendar. A proposed diagram consists of rice field segmentation (A), phenological computation (B) and analysis (C), rice crop calendar (D). The proposed diagram can be used to classify single and double crop cycles, which are the major types of cultivated areas in Thailand. A significant information for yield estimation model, start and end of growing season (SOS, EOS), can also be determined by using our approach.","tags":[],"title":"Rice crop calendar based on phenology analysis from time-series images","type":"publication"},{"authors":["Natur Soontranon","Panu Srestasathiern","Siam Lawawirojwong"],"categories":null,"content":"","date":1439424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1439424000,"objectID":"33721be3e777af35cb4a378860339140","permalink":"https://pnstst.github.io/publication/soontranon-2015b/","publishdate":"2015-08-13T00:00:00Z","relpermalink":"/publication/soontranon-2015b/","section":"publication","summary":" In Thailand, there are several types of (tangible) cultural heritages. This work focuses on 3D modeling of the heritage objects from multi-views images. The images are acquired by using a DSLR camera which costs around $1,500 (camera and lens). Comparing with a 3D laser scanner, the camera is cheaper and lighter than the 3D scanner. Hence, the camera is available for public users and convenient for accessing narrow areas. The acquired images consist of various sculptures and architectures in Wat-Pho which is a Buddhist temple located behind the Grand Palace (Bangkok, Thailand). Wat-Pho is known as temple of the reclining Buddha and the birthplace of traditional Thai massage. To compute the 3D models, a diagram is separated into following steps; Data acquisition, Image matching, Image calibration and orientation, Dense matching and Point cloud processing. For the initial work, small heritages less than 3 meters height are considered for the experimental results. A set of multi-views images of an interested object is used as input data for 3D modeling. In our experiments, 3D models are obtained from MICMAC (open source) software developed by IGN, France. The output of 3D models will be represented by using standard formats of 3D point clouds and triangulated surfaces such as .ply, .off, .obj, etc. To compute for the efficient 3D models, post-processing techniques are required for the final results e.g. noise reduction, surface simplification and reconstruction. The reconstructed 3D models can be provided for public access such as website, DVD, printed materials. The high accurate 3D models can also be used as reference data of the heritage objects that must be restored due to deterioration of a lifetime, natural disasters, etc.","tags":[],"title":"3D modeling from multi-views images for cultural heritage in wat-Pho, Thailand","type":"publication"},{"authors":["Narut Soontranon","Siam Lawawirojwong","Panwadee Tangpattanakul","Panu Srestasathiern","Preesan Rakwatin"],"categories":null,"content":"","date":1438387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438387200,"objectID":"66fba93ad8de2b60ba23f85133a710f7","permalink":"https://pnstst.github.io/publication/soontranon-2015/","publishdate":"2015-08-01T00:00:00Z","relpermalink":"/publication/soontranon-2015/","section":"publication","summary":"Rice is the most significant economic crops in Thailand, which is required to monitor and estimate cultivated area in a wide region. Satellite images are often used to analyze for classifying the paddy area. To validate the results, it is necessary to have ground collection data. For the ground data, instead of field staffs, an equipment called Field Server (FS) has been installed and used to obtain daily images on the paddy field. In this paper, the comparative results of satellite and FS images are experimented in order to understand the correlation between two platforms. Based on the vegetation indices, satellite and FS images are computed for two phenological curves in an observation period. Two curves are compared by using a simple linear regression method (R squared), which can be described the correlation between each other. The phenological curve obtained from FS images is used as a reference. To determine the crop cycle on the satellite images (MODIS), we found that NDVI is more efficient than Excessive Green (ExG) index.","tags":[],"title":"Comparative results of phenology obtained from satellite and ground observation images on paddy field","type":"publication"},{"authors":["Preesan Rakwatin","Saran Suwannachatkul1","Narut Soontranon1","Siam Lawavirojwong1","Panwadee Tangpattanakul1","Panu Srestasathiern1"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"acde929ea99e75eb00fa5d5907e31380","permalink":"https://pnstst.github.io/publication/rakwatin-2015/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/rakwatin-2015/","section":"publication","summary":"Rice is the staple food in Thailand. Thailand rice cultivation in can be divided into two classes, rain-fed rice and irrigated rice. Thailand, rice fields are located mostly in the central and north-east regions which are low-land and flat areas. Accurate paddy field monitoring is critical for yield estimating missions. Field survey is of limited utility to describe rice phenology across large areas. Remote sensing is probably the most reliable measurement tool for accurate rice monitoring over large areas. In this study, we compared time series MODIS and PROBA-V - normalized difference vegetation index (NDVI) and Radarsat-2 dual-polarization images that covered paddy area in Thailand. MODIS-derived NDVI derived from 8-day composite. Whereas, Radarsat-2 image were acquired every 24 days repeat cycle. From the result scatter plot, the MODIS and PROBA-V show the high result in R2 values (0.9027). This means that the characteristics of MODIS NDVI and PROBA-V NDVI are almost the same. For comparing the data with different sensor, PROBA-V NDVI with RADARSAT-2 HH Backscatter show the highest R2 values (0.7161) which show a good sign that both data from optical satellite and SAR satellite capable to fuse together. ","tags":[],"title":"A comparison of RadarSat-2 and MODIS vegetation indices for rice crop phenology in Thailand","type":"publication"},{"authors":["Narut Soontranon","Panu Srestasathiern","Preesan Rakwatin"],"categories":null,"content":"","date":1413849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413849600,"objectID":"a8cf1b7b2a8bc51e3af28118275b89d8","permalink":"https://pnstst.github.io/publication/soontranon-2014c/","publishdate":"2014-10-21T00:00:00Z","relpermalink":"/publication/soontranon-2014c/","section":"publication","summary":"In this study, an algorithm is proposed to determine the duration in a rice crop cycle based on texture analysis. During an observation period in 2013, daily images were acquired from a still camera installed at a paddy field. Given a set of time-series images, the texture analysis is used to classify different stages of the rice growing. Regarding the hypothesis, the rice crop cycle can be separated into three paddy stages; starting (coarse texture), midpoint (fine texture) and ending (no texture). The texture analysis is based on Gray Level Co-occurrence Matrix (GLCM) with multi-distances. The proposed diagram can be described as follows. Initially, the paddy region is selected as an area of interest (AOI). The selected AOI is computed for a vegetation index, Excessive Green (ExG), and enhanced by using histogram equalization method. Then, the enhanced image is used for generating co-occurrence matrix in order to describe the texture feature. Statistical property (contrast) is analyzed and measured on the co-occurrence matrix for paddy stage classification. The results show that our proposed diagram can be efficiently used to determine the duration of paddy stages. To obtain more efficient results, our perspective work will consider other features such as color, edge, shape, etc.","tags":[],"title":"Rice crop phenology using texture analysis on time-series images obtained from still camera","type":"publication"},{"authors":["Panu Srestasathiern","Narut Soontranon","Preesan Rakwatin"],"categories":null,"content":"","date":1413849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413849600,"objectID":"5dc4d499c57269294aaeee8d78f5df10","permalink":"https://pnstst.github.io/publication/srestasathiern-2014c/","publishdate":"2014-10-21T00:00:00Z","relpermalink":"/publication/srestasathiern-2014c/","section":"publication","summary":"Field server is the equipment used for monitoring the rice field using RGB images acquired daily e.g., at 10 a.m. or 2p.m. The obtained RGB images are used to compute the vegetation indices which are used to analyze and monitor the rice growing stage and health. However, the reflectance information on each RGB channel is contaminated by illumination changing which can affect the computed vegetation indices such as the fluctuation of their time series. In this paper, I proposed to use the intrinsic images of the scene to compute the vegetation indices. The term intrinsic images was proposed by Barrow and Tenenbaum (Barrow and Tenenbaum (1978)) as the midlevel description of a scene. Precisely, an image of the scene is defined as the product of an illumination image and a reflectance image. Assuming that the reflectance of the scene is constant and illumination changes, the decomposition of the illumination and reflectance images uses the image sequences of rice paddy field recorded at different times. The decomposed reflectance image is then independent from the illumination change and used for computing the vegetation index. To test the performance of the proposed method, the images used was from the rice fields located in Suphanburi provinces, Thailand. The time series of the vegetation index computed from the decomposed intrinsic image was much smoother than that of fluctuating vegetation index computed from original RGB images. The smoother time series of vegetation index can improve the interpretation of rice growing state. ","tags":[],"title":"Rice field monitoring using intrinsic image decomposed from field server imagery","type":"publication"},{"authors":["Panu Srestasathiern","Preesan Rakwatin"],"categories":null,"content":"","date":1413158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413158400,"objectID":"2632e669567c91380b656c8666a345a8","permalink":"https://pnstst.github.io/publication/srestasathiern-2014b/","publishdate":"2014-10-13T00:00:00Z","relpermalink":"/publication/srestasathiern-2014b/","section":"publication","summary":"Oil palm tree is an important cash crop in Thailand. To maximize the productivity from planting, oil palm plantation managers need to know the number of oil palm trees in the plantation area. In order to obtain this information, an approach for palm tree detection using high resolution satellite images is proposed. This approach makes it possible to count the number of oil palm trees in a plantation. The process begins with the selection of the vegetation index having the highest discriminating power between oil palm trees and background. The index having highest discriminating power is then used as the primary feature for palm tree detection. We hypothesize that oil palm trees are located at the local peak within the oil palm area. To enhance the separability between oil palm tree crowns and background, the rank transformation is applied to the index image. The local peak on the enhanced index image is then detected by using the non-maximal suppression algorithm. Since both rank transformation and non-maximal suppression are window based, semi-variogram analysis is used to determine the appropriate window size. The performance of the proposed method was tested on high resolution satellite images. In general, our approach uses produced very accurate results, e.g., about 90 percent detection rate when compared with manual labeling","tags":[],"title":"Oil palm tree detection with high resolution multi-spectral satellite imagery","type":"publication"},{"authors":["Panu Srestasathiern","Narut Soontranon"],"categories":null,"content":"","date":1409875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409875200,"objectID":"ca6c0e6390f3097546a7936d82edfded","permalink":"https://pnstst.github.io/publication/srestasathiern-2014/","publishdate":"2014-08-01T00:00:00Z","relpermalink":"/publication/srestasathiern-2014/","section":"publication","summary":"In this paper, a novel method for the fish-eye lens calibration is presented. The method required only a 2D calibration plane containing straight lines i.e., checker board pattern without a priori knowing the poses of camera with respect to the calibration plane. The image of a line obtained from fish-eye lenses is a conic section. The proposed calibration method uses raw edges, which are pixels of the image line segments, in stead of using curves obtained from fitting conic to image edges. Using raw edges is more flexible and reliable than using conic section because the result from conic fitting can be unstable. The camera model used in this work is radially symmetric model i.e., bivariate non-linear function. However, this approach can use other single view point camera models. The geometric constraint used for calibrating the camera is based on the coincidence between point and line on calibration plane. The performance of the proposed calibration algorithm was assessed using simulated and real data.","tags":[],"title":"A novel camera calibration method for fish-eye lenses using line features","type":"publication"},{"authors":["Narut Soontranon","Panwadee Tangpattanakul","Panu Srestasathiern","Preesan Rakwatin"],"categories":null,"content":"","date":1408838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1408838400,"objectID":"bf0a2992618c741dbfd57b987a5a7eb1","permalink":"https://pnstst.github.io/publication/soontranon-2014b/","publishdate":"2015-01-19T00:00:00Z","relpermalink":"/publication/soontranon-2014b/","section":"publication","summary":"The paper presents an agricultural monitoring system developed for Thailand. Various species of plants have been directly observed from the agricultural fields which mainly consist of economic crops of Thailand such as rice, cassava, rubber, sugar cane, corn, etc. An equipment used to obtain the data is called field server, which has been installed at the observed field for a long period. The collected data is separated to two parts: daily images (acquired twice per day) and weather information (recorded every five minutes). The weather information is as follows: temperature, rain volume, light density, humidity, soil moisture, wind speed and direction. Since the beginning of 2014, twenty-four field servers have been deployed in every region of Thailand. The data from the field servers is uploaded to a central server. Users can access and obtain the data via a web browser. Given the images and weather information (temperature and soil moisture), the data recorded from paddy fields is preliminarily analyzed as a guideline for further development.","tags":[],"title":"An agricultural monitoring system: Field server data collection and analysis on paddy field","type":"publication"},{"authors":["Pakapoj Tulsuk","Panu Srestasathiern","Miti Ruchanurucks","Teera Phatrapornnant","Hiroshi Nagahashi"],"categories":null,"content":"","date":1402185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1402185600,"objectID":"4a80b1cfc001991afe7810e05178409c","permalink":"https://pnstst.github.io/publication/tulsuk-2014b/","publishdate":"2014-07-17T00:00:00Z","relpermalink":"/publication/tulsuk-2014b/","section":"publication","summary":"This paper presents a novel method for extrinsic parameters estimation of a single line scan LiDAR and a camera. Using a checkerboard, the calibration setup is simple and practical. Particularly, the proposed calibration method is based on resolving geometry of the checkerboard that visible to the camera and the LiDAR. The calibration setup geometry is described by planes, lines and points. Our novelty is a new hypothesis of the geometry which is the orthogonal distances between LiDAR points and the line from the intersection between the checkerboard and LiDAR scan plane. To evaluate the performance of the proposed method, we compared our proposed method with the state of the art method i.e. Zhang and Pless [1]. The experimental results showed that the proposed method yielded better results.","tags":[],"title":"A novel method for extrinsic parameters estimation between a single-line scan LiDAR and a camera","type":"publication"},{"authors":["Narut Soontranon","Panu Srestasathiern","Preesan Rakwatin"],"categories":null,"content":"","date":1400025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1400025600,"objectID":"11528f902af58a207b5dfb1a948c439a","permalink":"https://pnstst.github.io/publication/soontranon-2014/","publishdate":"2014-07-17T00:00:00Z","relpermalink":"/publication/soontranon-2014/","section":"publication","summary":"In this paper, a software program is developed to monitor rice growing stages. Images are required as input data for the software. Using field server equipment, the images are obtained from two rice fields located in Suphanburi and Roi Et provinces, Thailand. Each daily image covers approximately 100x100 m^2 recorded by 720x480 pixels. Typically, a rice growing cycle is separated by three stages; seedling, tillering and heading. To define each stage, vegetation index is used for monitoring and analysing. In the prototype software, the vegetation index is computed from visible RGB channels. Our proposed diagram is described by three steps. a) Rice field segmentation is an initial step used to segment rice field region from the other regions (landscape, sky). b) Vegetation index computation is a measurement, which measures the levels of live green plants on the rice field region. c) Graph analysis is an algorithm used to determine and separate the rice growing stages. The experiments compared three vegetation indices; ExG-Excessive Green, NGRDI-Normalized Green Red Difference Index and ExGR-difference of ExG and ExR (Excessive Red). Relying on the images obtained from the field server, we found that the rice growing stages are able to monitor by using ExG index which is more efficient than the other two.","tags":[],"title":"Rice growing stage monitoring in small-scale region using ExG vegetation index","type":"publication"},{"authors":["Pakapoj Tulsuk","Panu Srestasathiern","Miti Ruchanurucks"],"categories":null,"content":"","date":1390435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1390435200,"objectID":"6e51ac1cbb7410f17f6e7afc076e4953","permalink":"https://pnstst.github.io/publication/tulsuk-2014a/","publishdate":"2014-01-23T00:00:00Z","relpermalink":"/publication/tulsuk-2014a/","section":"publication","summary":"This paper presents a feature extraction procedure of 2D-scan Lidars data. The procedure is a detection of a line and its correspondent points. Detected features are used to calibrate the Lidar and a camera for their orientation and position. We applied Hough transform and RANSAC to estimate the line and its correspondent points. And we also applied Zhang and Pless algorithm to calibrate their orientation and position.","tags":[],"title":"Feature extraction of line-scan LIDAR for calibrating camera and LIDAR","type":"publication"},{"authors":null,"categories":null,"content":"The aims of my research is to extract useful information for agricultural management. Particularly, geo-spatial data such as satellite imagery, image from ground sensor and UAV imagery are used to extract agricultural insight. An example is the count the amount of tree in plantations areas such as palm tree. With the constant revisit time of satellite, the time series data of crop can be extracted such as crop phenology and crop cultivation date.\n","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"a9e297fb24b5335ff04180a4db0f6359","permalink":"https://pnstst.github.io/project/agriculture/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/project/agriculture/","section":"project","summary":" ","tags":[],"title":"Agricultural monitoring","type":"project"},{"authors":null,"categories":null,"content":"The aim of computer vision is to enable computer or system to extract meaningful information from digital images or other visual inputs. Geometric computer vision concentrate on derive geometric information from object of interests. The research topics Includes but not limited to geometric sensor calibration, sensor fusion, 3D reconstruction, and pattern recognition invaraint to camera view points.\n","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"25c3c12a61bb251cccbdf1a4f25d16cc","permalink":"https://pnstst.github.io/project/geometric-cv/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/project/geometric-cv/","section":"project","summary":" ","tags":[],"title":"Geometric computer vision","type":"project"},{"authors":["Panu Srestasathiern","Preesan Rakwatin"],"categories":null,"content":"","date":1382572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1382572800,"objectID":"520b671cf5fe00f26f49a7f5b1e5380a","permalink":"https://pnstst.github.io/publication/srestasathiern-2013/","publishdate":"2013-10-24T00:00:00Z","relpermalink":"/publication/srestasathiern-2013/","section":"publication","summary":"Oil palm tree is one of the economic crops of Thailand. In order to maximize the productivity from planting, appropriate management of oil palm farm is highly required. The basic required information for oil palm tree management is the amount of oil palm tree in the plantation area. In order to obtain such information, we proposed an approach for palm tree detection. As a result, the amount of oil palm trees in an area can be counted. The proposed method is based on the use of multispectral imagery for the computation of vegetation index i.e. NDVI. The rank transformation is then applied to enhance the discrimination between the oil palm tree and the background. To detect oil palm trees, we hypothesize that the location of the oil palm trees are located at the local peak of the vegetation index image after the rank transformation is applied. To perform the local peak detection, the non-maximal suppression is employed. The performance of the proposed method is tested on several multispectral images and the accuracy of the detection result is reported.","tags":[],"title":"An approach for oil palm tree detection using multispectral imagery","type":"publication"},{"authors":null,"categories":null,"content":"Remotely sensed image analysis is the process of interpreting satellite imagery. In other words, the aims of image analysis is to locating, recognizing objects and other higher level features in an satellite image. My research on remotely sensed image analysis including image classification, change detection.\n","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"cdba993deb1bb50b36a39c4978cc3ed3","permalink":"https://pnstst.github.io/project/image-analysis/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/project/image-analysis/","section":"project","summary":" ","tags":[],"title":"Remotely sensed image analysis","type":"project"},{"authors":["Panu Srestasathiern","Alper Yilmaz"],"categories":null,"content":"","date":1312502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1312502400,"objectID":"2f10573e5cef15b8124a9517cda9e1ef","permalink":"https://pnstst.github.io/publication/srestasathiern-2011/","publishdate":"2011-08-05T00:00:00Z","relpermalink":"/publication/srestasathiern-2011/","section":"publication","summary":"This paper introduces a new representation for planar objects which is invariant to projective transformation. Proposed representation relies on a new shape basis which we refer to as the conic basis. The conic basis takes conic-section coefficients as its dimensions and represents the object as a convex combination of conic-sections. Pairs of conic-sections in this new basis and their projective invariants provides the proposed view invariant representation. We hypothesize that two projectively transformed versions of an object result in the same representation. We show that our hypothesis provides promising recognition performance when we use the nearest neighbor rule to match projectively deformed objects.","tags":[],"title":"Planar shape representation and matching under projective transformation","type":"publication"},{"authors":["Panu Srestasathiern","Alper Yilmaz"],"categories":null,"content":"","date":1228694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1228694400,"objectID":"1de8d052f6150966f44fff7e5a66d9c5","permalink":"https://pnstst.github.io/publication/srestasathiern-2008/","publishdate":"2008-12-08T00:00:00Z","relpermalink":"/publication/srestasathiern-2008/","section":"publication","summary":"This paper introduces a method for the recognition planar objects under projective geometry. Our method is based on a similarity measure invariant to projective transform. The proposed similarity measure utilizes the distribution of the projective relations between the conic section pairs, which are estimated from the object¬øs shape. We conjecture that given two objects of the same type, which are viewed from different viewpoints generate similar histograms, such that their difference is smaller than the histograms generated from other object types. The proposed measure has shown promising performance on the Brown shape database.","tags":[],"title":"View invariant object recognition","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://pnstst.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]